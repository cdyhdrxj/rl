{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3df698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3fa45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Сетка n на m  \n",
    "Одна клетка = 0 (пустая клетка) или 1 (препятствие)  \n",
    "Начальная клетка - A, конечная клетка (заключительное состояние) - B\n",
    "\n",
    "**Действия:**  \n",
    "переход в одну из соседних по стороне клеток\n",
    "\n",
    "**Динамика среды:**  \n",
    "- переход выполняется, если клетка пуста\n",
    "- не выполняется (агент остается в той же клетке), если препятствие или зашли за границы сетки\n",
    "- переходы из конечной клетки не выполняются\n",
    "- вознаграждение: -1 за перемещение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0691870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сетка\n",
    "grid = [\n",
    "    [0, 0, 0, 1, 0, 0],\n",
    "    [0, 1, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 1, 0]\n",
    "]\n",
    "\n",
    "# Размерности сетки\n",
    "n, m = len(grid), len(grid[0])\n",
    "\n",
    "# Конечная клетка - заключительное состояние\n",
    "B = (n-1, m-1)\n",
    "\n",
    "# Действия (изменения координат)\n",
    "actions = [\n",
    "    (-1, 0), (0, 1), (1, 0), (0, -1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b16065e",
   "metadata": {},
   "source": [
    "### 4.1 ДП. Оценивание стратегии"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1054855",
   "metadata": {},
   "source": [
    "Стратегия: переход в одну из соседних клеток с равной вероятностью $\\frac{1}{4}$  \n",
    "\n",
    "Построить оценку функции ценности для состояний при использовании этой стратегии  \n",
    "\n",
    "Обесценивание не применяется ($\\gamma = 1$)  \n",
    "Т.о. ценность состояния = отрицательное мат. ожидание длины пути до заключительного состояния при использовании данной стратегии  \n",
    "Недостаток - если из каких-то состояний не достижимо заключительное состояние, ценность будет бесконечно увеличиваться, алгоритм не сойдётся к $\\nu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb415215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 1e-05\n",
      "Количество итераций: 2164\n",
      "Сетка:\n",
      "   0  1  2  3  4  5\n",
      "0  0  0  0  1  0  0\n",
      "1  0  1  1  0  0  0\n",
      "2  0  0  0  0  0  0\n",
      "3  0  0  1  0  0  0\n",
      "4  0  0  0  1  1  0\n",
      "Оценка функции ценности состояний:\n",
      "         0        1        2        3        4        5\n",
      "0 -189.454 -192.454 -193.454    0.000 -104.064 -102.300\n",
      "1 -184.454    0.000    0.000 -109.808 -103.828  -98.536\n",
      "2 -177.454 -165.788 -140.788 -113.789  -98.904  -86.480\n",
      "3 -179.121 -176.121    0.000 -101.654  -87.519  -59.000\n",
      "4 -180.788 -180.454 -181.454    0.000    0.000    0.000\n"
     ]
    }
   ],
   "source": [
    "# Стратегия\n",
    "base_pi = [[(1/4, 1/4, 1/4, 1/4) for _ in range(m)] for _ in range(n)]\n",
    "\n",
    "# Точность оценки\n",
    "theta = 1e-5\n",
    "\n",
    "# Алгоритм итеративного оценивания стратегии для оценивания V\n",
    "def dp_value_function(pi, V):\n",
    "    iter_count = 0\n",
    "    while True:\n",
    "        iter_count += 1\n",
    "        delta = 0\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                v = V[i][j]\n",
    "                V[i][j] = 0\n",
    "                # Не считаем ценность для закл. состояния и клеток с препятствиями\n",
    "                if grid[i][j] == 1 or (i, j) == B:\n",
    "                    continue\n",
    "                for action_i, (di, dj) in enumerate(actions):\n",
    "                    i_next, j_next = i + di, j + dj\n",
    "                    # Переходим в соседнюю клетку\n",
    "                    if 0 <= i_next < n and 0 <= j_next < m and grid[i_next][j_next] == 0:\n",
    "                        V[i][j] += pi[i][j][action_i] * (-1 + V[i_next][j_next])\n",
    "                    # Остаёмся в текущей клетке\n",
    "                    else:\n",
    "                        V[i][j] += pi[i][j][action_i] * (0 + v)   # Правильно ли обновлять оценку состояния через пред. оценку этого состояния?\n",
    "                delta = max(delta, abs(v - V[i][j]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    print(f'Точность: {theta}')\n",
    "    print(f'Количество итераций: {iter_count}')\n",
    "    return V\n",
    "\n",
    "V = [[0] * m for _ in range(n)]\n",
    "V = dp_value_function(base_pi, V)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.precision', 3)\n",
    "print('Сетка:')\n",
    "print(pd.DataFrame(grid))\n",
    "print('Оценка функции ценности состояний:')\n",
    "print(pd.DataFrame(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6babc880",
   "metadata": {},
   "source": [
    "### 4.3 ДП. Итерация по стратегиям"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f262cc4",
   "metadata": {},
   "source": [
    "Найдём оптимальную стратегию для задачи выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "766df320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 1e-05\n",
      "Количество итераций: 2164\n",
      "Точность: 1e-05\n",
      "Количество итераций: 33711\n",
      "Точность: 1e-05\n",
      "Количество итераций: 7\n",
      "Точность: 1e-05\n",
      "Количество итераций: 1\n",
      "Оценка функции ценности состояний для опт. стратегии:\n",
      "       0       1       2      3      4      5\n",
      "0 -9.007 -10.008 -11.008  0.000 -5.004 -4.004\n",
      "1 -8.007   0.000   0.000 -5.004 -4.004 -3.003\n",
      "2 -7.006  -6.005  -5.004 -4.004 -3.003 -2.002\n",
      "3 -8.007  -7.006   0.000 -3.003 -2.002 -1.001\n",
      "4 -9.007  -8.007  -9.007  0.000  0.000  0.000\n",
      "Оптимальная стратегия:\n",
      "                                      0                                     1  \\\n",
      "0  (0.00025, 0.00025, 0.99925, 0.00025)  (0.00025, 0.00025, 0.00025, 0.99925)   \n",
      "1  (0.00025, 0.00025, 0.99925, 0.00025)              (0.25, 0.25, 0.25, 0.25)   \n",
      "2  (0.00025, 0.99925, 0.00025, 0.00025)  (0.00025, 0.99925, 0.00025, 0.00025)   \n",
      "3  (0.99925, 0.00025, 0.00025, 0.00025)  (0.99925, 0.00025, 0.00025, 0.00025)   \n",
      "4  (0.99925, 0.00025, 0.00025, 0.00025)  (0.99925, 0.00025, 0.00025, 0.00025)   \n",
      "\n",
      "                                      2                                     3  \\\n",
      "0  (0.00025, 0.00025, 0.00025, 0.99925)              (0.25, 0.25, 0.25, 0.25)   \n",
      "1              (0.25, 0.25, 0.25, 0.25)  (0.00025, 0.00025, 0.99925, 0.00025)   \n",
      "2  (0.00025, 0.99925, 0.00025, 0.00025)  (0.00025, 0.00025, 0.99925, 0.00025)   \n",
      "3              (0.25, 0.25, 0.25, 0.25)  (0.00025, 0.99925, 0.00025, 0.00025)   \n",
      "4  (0.99925, 0.00025, 0.00025, 0.00025)              (0.25, 0.25, 0.25, 0.25)   \n",
      "\n",
      "                                      4                                     5  \n",
      "0  (0.00025, 0.99925, 0.00025, 0.00025)  (0.00025, 0.00025, 0.99925, 0.00025)  \n",
      "1  (0.00025, 0.00025, 0.99925, 0.00025)  (0.00025, 0.00025, 0.99925, 0.00025)  \n",
      "2  (0.00025, 0.00025, 0.99925, 0.00025)  (0.00025, 0.00025, 0.99925, 0.00025)  \n",
      "3  (0.00025, 0.99925, 0.00025, 0.00025)  (0.00025, 0.00025, 0.99925, 0.00025)  \n",
      "4              (0.25, 0.25, 0.25, 0.25)              (0.25, 0.25, 0.25, 0.25)  \n"
     ]
    }
   ],
   "source": [
    "pi = base_pi.copy()\n",
    "V = [[0] * m for _ in range(n)]\n",
    "epsilon = 0.001\n",
    "\n",
    "def dp_optimal_strategy(pi, V):\n",
    "    while True:\n",
    "        # Оценивание стратегии\n",
    "        V = dp_value_function(pi, V)\n",
    "\n",
    "        # Улучшение стратегии\n",
    "        policy_stable = True\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                if grid[i][j] == 1 or (i, j) == B:\n",
    "                    continue\n",
    "\n",
    "                old_action = pi[i][j]\n",
    "\n",
    "                q_values = []\n",
    "                for (di, dj) in actions:\n",
    "                    i_next, j_next = i + di, j + dj\n",
    "                    if 0 <= i_next < n and 0 <= j_next < m and grid[i_next][j_next] == 0:\n",
    "                        q = -1 + V[i_next][j_next]\n",
    "                    else:\n",
    "                        q = 0 + V[i][j]\n",
    "                    q_values.append(q)\n",
    "\n",
    "                best_action = q_values.index(max(q_values))\n",
    "\n",
    "                # Эпсилон-мягкая стратегия\n",
    "                new_pi = [epsilon / len(actions)] * len(actions)\n",
    "                new_pi[best_action] = 1 - epsilon + (epsilon / len(actions))\n",
    "                pi[i][j] = tuple(new_pi)\n",
    "\n",
    "                if old_action != pi[i][j]:\n",
    "                    policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            return pi, V\n",
    "\n",
    "pi_opt, V_opt = dp_optimal_strategy(pi, V)\n",
    "\n",
    "print('Оценка функции ценности состояний для опт. стратегии:')\n",
    "print(pd.DataFrame(V))\n",
    "\n",
    "print('Оптимальная стратегия:')\n",
    "print(pd.DataFrame(pi_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baf051a",
   "metadata": {},
   "source": [
    "#### Новая модель:\n",
    "\n",
    "Пусть у свободной клетки будет значение $x$:  \n",
    "$0 \\le x < 1$ - \"степень пробуксовки\" агента в клетке  \n",
    "x = вероятность остаться в клетке при попытке выхода из неё\n",
    "\n",
    "$P[(i', j') | (i, j), a] = 1 - x$  \n",
    "$P[(i, j) | (i, j), a] = x$\n",
    "\n",
    "Будем давать вознаграждение -5 за действие в клетке с $0 < x < 1$, если агент остался в клетке\n",
    "(попробовать давать вознаграждение зависящее от x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033bb87d",
   "metadata": {},
   "source": [
    "Пусть $x$ имеет точность до 1 знака после запятой  \n",
    "Будем хранить в сетке целые значения $10x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e199f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сетка\n",
    "grid = [\n",
    "    [0, 0, 1, 1, 0, 0],\n",
    "    [0, 5, 5, 5, 0, 0],\n",
    "    [0, 5, 8, 5, 0, 10],\n",
    "    [0, 5, 5, 5, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "# Размерности сетки\n",
    "n, m = len(grid), len(grid[0])\n",
    "\n",
    "# Конечная клетка - заключительное состояние\n",
    "B = (n-1, m-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d04fb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 1e-05\n",
      "Количество итераций: 1918\n",
      "Сетка:\n",
      "   0  1  2  3  4   5\n",
      "0  0  0  1  1  0   0\n",
      "1  0  5  5  5  0   0\n",
      "2  0  5  8  5  0  10\n",
      "3  0  5  5  5  0   0\n",
      "4  0  0  0  0  0   0\n",
      "Оценка функции ценности состояний:\n",
      "         0        1        2        3        4        5\n",
      "0 -483.089 -481.769 -473.310 -456.747 -441.056 -437.883\n",
      "1 -482.410 -485.908 -476.748 -451.208 -425.538 -432.710\n",
      "2 -475.231 -478.706 -472.564 -421.800 -373.176    0.000\n",
      "3 -461.579 -457.120 -429.002 -366.254 -269.191 -135.595\n",
      "4 -449.386 -435.193 -396.072 -321.021 -197.737    0.000\n"
     ]
    }
   ],
   "source": [
    "# Стратегия\n",
    "base_pi = [[(1/4, 1/4, 1/4, 1/4) for _ in range(m)] for _ in range(n)]\n",
    "\n",
    "# Точность оценки\n",
    "theta = 1e-5\n",
    "\n",
    "# Алгоритм итеративного оценивания стратегии для оценивания V\n",
    "def dp_value_function(pi, V):\n",
    "    iter_count = 0\n",
    "    while True:\n",
    "        iter_count += 1\n",
    "        delta = 0\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                v = V[i][j]\n",
    "                V[i][j] = 0\n",
    "                # Не считаем ценность для закл. состояния и клеток с препятствиями\n",
    "                if grid[i][j] == 10 or (i, j) == B:\n",
    "                    continue\n",
    "                for action_i, (di, dj) in enumerate(actions):\n",
    "                    i_next, j_next = i + di, j + dj\n",
    "                    # Переходим в соседнюю клетку\n",
    "                    if 0 <= i_next < n and 0 <= j_next < m and grid[i_next][j_next] != 10:\n",
    "                        V[i][j] += pi[i][j][action_i] * (1 - grid[i][j] / 10) * (-1 + V[i_next][j_next])\n",
    "                        V[i][j] += pi[i][j][action_i] * (grid[i][j] / 10) * (-5 + v)\n",
    "                    # Остаёмся в текущей клетке\n",
    "                    else:\n",
    "                        V[i][j] += pi[i][j][action_i] * (0 + v)\n",
    "                delta = max(delta, abs(v - V[i][j]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    print(f'Точность: {theta}')\n",
    "    print(f'Количество итераций: {iter_count}')\n",
    "    return V\n",
    "\n",
    "V = [[0] * m for _ in range(n)]\n",
    "V = dp_value_function(base_pi, V)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.precision', 3)\n",
    "print('Сетка:')\n",
    "print(pd.DataFrame(grid))\n",
    "print('Оценка функции ценности состояний:')\n",
    "print(pd.DataFrame(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e74d931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 1e-05\n",
      "Количество итераций: 1918\n",
      "Точность: 1e-05\n",
      "Количество итераций: 78\n",
      "Точность: 1e-05\n",
      "Количество итераций: 55\n",
      "Оценка функции ценности состояний для опт. стратегии:\n",
      "       0       1       2       3      4      5\n",
      "0 -9.015  -9.126  -8.125  -6.566 -5.009 -6.008\n",
      "1 -8.015 -14.018 -14.132 -10.012 -4.008 -5.008\n",
      "2 -7.012 -13.021 -30.017  -9.015 -3.005  0.000\n",
      "3 -6.010 -10.012  -9.015  -8.006 -2.003 -1.001\n",
      "4 -5.008  -4.008  -3.006  -2.003 -1.001  0.000\n",
      "Оптимальная стратегия:\n",
      "                                      0                                     1  \\\n",
      "0  (0.00025, 0.00025, 0.99925, 0.00025)  (0.00025, 0.99925, 0.00025, 0.00025)   \n",
      "1  (0.00025, 0.00025, 0.99925, 0.00025)  (0.00025, 0.00025, 0.00025, 0.99925)   \n",
      "2  (0.00025, 0.00025, 0.99925, 0.00025)  (0.00025, 0.00025, 0.00025, 0.99925)   \n",
      "3  (0.00025, 0.00025, 0.99925, 0.00025)  (0.00025, 0.00025, 0.99925, 0.00025)   \n",
      "4  (0.00025, 0.99925, 0.00025, 0.00025)  (0.00025, 0.99925, 0.00025, 0.00025)   \n",
      "\n",
      "                                      2                                     3  \\\n",
      "0  (0.00025, 0.99925, 0.00025, 0.00025)  (0.00025, 0.99925, 0.00025, 0.00025)   \n",
      "1  (0.99925, 0.00025, 0.00025, 0.00025)  (0.00025, 0.99925, 0.00025, 0.00025)   \n",
      "2  (0.00025, 0.99925, 0.00025, 0.00025)  (0.00025, 0.99925, 0.00025, 0.00025)   \n",
      "3  (0.00025, 0.00025, 0.99925, 0.00025)  (0.00025, 0.99925, 0.00025, 0.00025)   \n",
      "4  (0.00025, 0.99925, 0.00025, 0.00025)  (0.00025, 0.99925, 0.00025, 0.00025)   \n",
      "\n",
      "                                      4                                     5  \n",
      "0  (0.00025, 0.00025, 0.99925, 0.00025)  (0.00025, 0.00025, 0.99925, 0.00025)  \n",
      "1  (0.00025, 0.00025, 0.99925, 0.00025)  (0.00025, 0.00025, 0.00025, 0.99925)  \n",
      "2  (0.00025, 0.00025, 0.99925, 0.00025)              (0.25, 0.25, 0.25, 0.25)  \n",
      "3  (0.00025, 0.99925, 0.00025, 0.00025)  (0.00025, 0.00025, 0.99925, 0.00025)  \n",
      "4  (0.00025, 0.99925, 0.00025, 0.00025)              (0.25, 0.25, 0.25, 0.25)  \n"
     ]
    }
   ],
   "source": [
    "pi = base_pi.copy()\n",
    "V = [[0] * m for _ in range(n)]\n",
    "epsilon = 0.001\n",
    "\n",
    "def dp_optimal_strategy(pi, V):\n",
    "    while True:\n",
    "        # Оценивание стратегии\n",
    "        V = dp_value_function(pi, V)\n",
    "\n",
    "        # Улучшение стратегии\n",
    "        policy_stable = True\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                if grid[i][j] == 10 or (i, j) == B:\n",
    "                    continue\n",
    "\n",
    "                old_action = pi[i][j]\n",
    "\n",
    "                q_values = []\n",
    "                for (di, dj) in actions:\n",
    "                    i_next, j_next = i + di, j + dj\n",
    "                    if 0 <= i_next < n and 0 <= j_next < m and grid[i_next][j_next] != 10:\n",
    "                        q = (1 - grid[i][j] / 10) * (-1 + V[i_next][j_next])\n",
    "                        q += (grid[i][j] / 10) * (-5 + V[i][j])\n",
    "                    else:\n",
    "                        q = 0 + V[i][j]\n",
    "                    q_values.append(q)\n",
    "\n",
    "                best_action = q_values.index(max(q_values))\n",
    "\n",
    "                # Эпсилон-мягкая стратегия\n",
    "                new_pi = [epsilon / len(actions)] * len(actions)\n",
    "                new_pi[best_action] = 1 - epsilon + (epsilon / len(actions))\n",
    "                pi[i][j] = tuple(new_pi)\n",
    "\n",
    "                if old_action != pi[i][j]:\n",
    "                    policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            return pi, V\n",
    "\n",
    "pi_opt, V_opt = dp_optimal_strategy(pi, V)\n",
    "\n",
    "print('Оценка функции ценности состояний для опт. стратегии:')\n",
    "print(pd.DataFrame(V))\n",
    "\n",
    "print('Оптимальная стратегия:')\n",
    "print(pd.DataFrame(pi_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848bef1d",
   "metadata": {},
   "source": [
    "Проблема переноса опыта: используя текущую модель, можно обучить агента проходить конкретную сетку, но не перенести накопленный опыт на другую сетку\n",
    "\n",
    "#### Новая модель:\n",
    "\n",
    "Пусть состояние среды будет не положение агента $(i, j)$ в сетке, а  \n",
    "$s$ = (характеристика ($x$) текущей клетки, характеристики соседних по стороне клеток, смещение от цели ($\\Delta i, \\Delta j$))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
